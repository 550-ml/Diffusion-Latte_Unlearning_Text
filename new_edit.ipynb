{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/wangtuo/Miniconda/envs/latte2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31034114614d47a084b6da6288074026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models.latte_t2v import LatteT2V  # 导入模型\n",
    "from diffusers.models import AutoencoderKL, AutoencoderKLTemporalDecoder\n",
    "from diffusers.schedulers import PNDMScheduler\n",
    "import torch\n",
    "from einops import repeat\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "pretrain_model_path = '/data2/wangtuo/workspace/model/Latte/hf_hub/models--maxin-cn--Latte/t2v_required_models'\n",
    "model_name = '/data2/wangtuo/workspace/model/Latte/hf_hub/models--maxin-cn--Latte/t2v.pt'\n",
    "device = \"cuda:1\"\n",
    "\n",
    "# T2v\n",
    "lattet2v_model = LatteT2V.from_pretrained_2d(pretrain_model_path, subfolder=\"transformer\", video_length = 16).to(device) \n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "checkpoint = torch.load(model_name, map_location=lambda storage, loc: storage)\n",
    "lattet2v_model.load_state_dict(checkpoint['model'])\n",
    "# 把pipeline加载明白\n",
    "\n",
    "from sample.pipeline_videogen import VideoGenPipeline\n",
    "\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(pretrain_model_path, subfolder=\"vae\", torch_dtype=torch.float16).to(device)\n",
    "vae.half()\n",
    "print(vae.dtype)\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrain_model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = T5EncoderModel.from_pretrained(pretrain_model_path, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(device)\n",
    "scheduler = PNDMScheduler.from_pretrained(pretrain_model_path, \n",
    "                                                  subfolder=\"scheduler\",\n",
    "                                                  beta_start=0.0001, \n",
    "                                                  beta_end=0.02, \n",
    "                                                  beta_schedule=\"linear\",\n",
    "                                                variance_type=\"learned_range\")\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "videogen_pipeline = VideoGenPipeline(vae=vae, \n",
    "                                text_encoder=text_encoder, \n",
    "                                tokenizer=tokenizer, \n",
    "                                scheduler=scheduler, \n",
    "                                transformer=lattet2v_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 1152])\n",
      "torch.Size([32, 6, 1152])\n"
     ]
    }
   ],
   "source": [
    "prompt = ['Rihanna']\n",
    "encoder_hidden_state, encoder_hidden_state_mask = videogen_pipeline.get_sentence_embedding  (prompt, \n",
    "                        video_length=16, \n",
    "                        height=512, \n",
    "                        width=512, \n",
    "                        num_inference_steps=100,\n",
    "                        guidance_scale=9.5,\n",
    "                        enable_temporal_attentions=True,\n",
    "                        num_images_per_prompt=1,\n",
    "                        mask_feature=True,\n",
    "                        enable_vae_temporal_decoder=False\n",
    "                        )\n",
    "prompt = ['Beyoncé']\n",
    "encoder_hidden_state2, encoder_hidden_state_mask2 = videogen_pipeline.get_sentence_embedding(prompt, \n",
    "                        video_length=16, \n",
    "                        height=512, \n",
    "                        width=512, \n",
    "                        num_inference_steps=100,\n",
    "                        guidance_scale=9.5,\n",
    "                        enable_temporal_attentions=True,\n",
    "                        num_images_per_prompt=1,\n",
    "                        mask_feature=True,\n",
    "                        enable_vae_temporal_decoder=False\n",
    "                        )\n",
    "print(encoder_hidden_state.shape)\n",
    "print(encoder_hidden_state2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0250, -0.2516,  0.1268,  0.0687,  0.1209, -0.1931, -0.2395,  0.1474,\n",
      "          0.2589,  0.0891],\n",
      "        [ 0.2702, -0.0277, -0.2992, -0.1014,  0.3698, -0.0843,  0.1775, -0.0253,\n",
      "         -0.1433,  0.1125]], device='cuda:1') tensor([[-0.0250, -0.2516,  0.1268,  0.0687,  0.1209, -0.1931, -0.2395,  0.1474,\n",
      "          0.2589,  0.0891],\n",
      "        [ 0.2702, -0.0277, -0.2992, -0.1014,  0.3698, -0.0843,  0.1775, -0.0253,\n",
      "         -0.1433,  0.1125]], device='cuda:1')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(encoder_hidden_state[16,:2,:10], encoder_hidden_state[17,:2,:10])\n",
    "atol=1e-8\n",
    "# 判断是否相等\n",
    "print(torch.allclose(encoder_hidden_state[16,0,512], encoder_hidden_state2[16,0,512], atol=1e-8))\n",
    "# # 计算两个张量的差异\n",
    "# difference = torch.abs(encoder_hidden_state - encoder_hidden_state2)\n",
    "\n",
    "# # 找到超过允许误差范围的位置\n",
    "# different_indices = torch.nonzero(difference > atol)\n",
    "\n",
    "# # 如果存在不同的值，打印不同的位置及其值\n",
    "# if different_indices.numel() > 0:\n",
    "#     print(f\"在误差范围 {atol} 内，不同的位置及其值如下：\")\n",
    "#     for idx in different_indices:\n",
    "#         idx = tuple(idx.tolist())\n",
    "#         print(f\"位置 {idx} - encoder_hidden_state: {encoder_hidden_state[idx]}, encoder_hidden_state2: {encoder_hidden_state2[idx]}\")\n",
    "# else:\n",
    "#     print(\"所有值在允许误差范围内相同。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些参数配置\n",
    "erase_scale = 0.1\n",
    "preserve_scale =0.1\n",
    "is_use_k = True\n",
    "old_texts_ = ['Biden is running']\n",
    "new_texts_ = ['Messi is running']\n",
    "ret_texts = ['A.J.Casson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_embedding(videogen_pipeline, prompt):\n",
    "    encoder_hidden_state, encoder_hidden_state_mask = videogen_pipeline.get_sentence_embedding(prompt, \n",
    "                        video_length=16, \n",
    "                        height=512, \n",
    "                        width=512, \n",
    "                        num_inference_steps=4,\n",
    "                        guidance_scale=7.5,\n",
    "                        enable_temporal_attentions=True,\n",
    "                        num_images_per_prompt=1,\n",
    "                        mask_feature=True,\n",
    "                        enable_vae_temporal_decoder=False\n",
    "                        )\n",
    "    return encoder_hidden_state[16,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatteT2V(\n",
      "  (pos_embed): PatchEmbed(\n",
      "    (proj): Conv2d(4, 1152, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0-27): 28 x BasicTransformerBlock(\n",
      "      (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
      "      (attn1): Attention(\n",
      "        (to_q): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_k): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_v): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
      "      (attn2): Attention(\n",
      "        (to_q): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_k): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_v): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (net): ModuleList(\n",
      "          (0): GELU(\n",
      "            (proj): Linear(in_features=1152, out_features=4608, bias=True)\n",
      "          )\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "          (2): Linear(in_features=4608, out_features=1152, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (temporal_transformer_blocks): ModuleList(\n",
      "    (0-27): 28 x BasicTransformerBlock_(\n",
      "      (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
      "      (attn1): Attention(\n",
      "        (to_q): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_k): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_v): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm3): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
      "      (ff): FeedForward(\n",
      "        (net): ModuleList(\n",
      "          (0): GELU(\n",
      "            (proj): Linear(in_features=1152, out_features=4608, bias=True)\n",
      "          )\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "          (2): Linear(in_features=4608, out_features=1152, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_out): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
      "  (proj_out): Linear(in_features=1152, out_features=32, bias=True)\n",
      "  (adaln_single): AdaLayerNormSingle(\n",
      "    (emb): CombinedTimestepSizeEmbeddings(\n",
      "      (time_proj): Timesteps()\n",
      "      (timestep_embedder): TimestepEmbedding(\n",
      "        (linear_1): Linear(in_features=256, out_features=1152, bias=True)\n",
      "        (act): SiLU()\n",
      "        (linear_2): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=1152, out_features=6912, bias=True)\n",
      "  )\n",
      "  (caption_projection): CaptionProjection(\n",
      "    (linear_1): Linear(in_features=4096, out_features=1152, bias=True)\n",
      "    (act_1): GELU(approximate='tanh')\n",
      "    (linear_2): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  )\n",
      ")\n",
      "transformer_blocks.0.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.1.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.2.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.3.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.4.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.5.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.6.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.7.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.8.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.9.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.10.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.11.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.12.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.13.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.14.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.15.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.16.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.17.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.18.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.19.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.20.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.21.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.22.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.23.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.24.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.25.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.26.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n",
      "transformer_blocks.27.ff.net.2\n",
      "Linear(in_features=4608, out_features=1152, bias=True)\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "find_model = videogen_pipeline.transformer.named_modules()\n",
    "print(videogen_pipeline.transformer)\n",
    "for name, module in videogen_pipeline.transformer.named_modules():\n",
    "    # 27个\n",
    "    if (\n",
    "        isinstance(module, torch.nn.Linear)\n",
    "        and \"ff.net\" in name\n",
    "        and not \"proj\" in name\n",
    "        and not \"temporal\" in name\n",
    "    ):\n",
    "        print(name)\n",
    "        print(module)\n",
    "        print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寻找交叉注意力\n",
      "Parameter containing:\n",
      "tensor([[ 0.0270, -0.0089,  0.0713,  ..., -0.0623, -0.0364,  0.0031],\n",
      "        [-0.0012,  0.0181,  0.0084,  ..., -0.0404,  0.0020,  0.0161],\n",
      "        [ 0.0142, -0.0662, -0.0083,  ..., -0.0006,  0.0634, -0.0402],\n",
      "        ...,\n",
      "        [-0.0273, -0.0215, -0.0196,  ..., -0.0344,  0.0239,  0.0042],\n",
      "        [ 0.0031, -0.0113, -0.0506,  ..., -0.0469,  0.0275, -0.0688],\n",
      "        [-0.0072,  0.0164,  0.0131,  ...,  0.0002,  0.0067,  0.0363]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "[LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True), LoRACompatibleLinear(in_features=1152, out_features=1152, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "transformers_block = lattet2v_model.transformer_blocks.named_children()\n",
    "ca_layers = []\n",
    "print(f'寻找交叉注意力')\n",
    "for transformer_block in transformers_block: \n",
    "    # 对于每一层transformer_block,选择basic_block\n",
    "    basic_transformer_block = transformer_block[1]\n",
    "    basic_transformer_layers = basic_transformer_block.named_children()\n",
    "    # 遍历，找到基本层\n",
    "    for layer in basic_transformer_layers:\n",
    "        if 'attn2' in layer[0]:\n",
    "            ca_layers.append(layer[1])\n",
    "            \n",
    "print(ca_layers[0].to_v.weight)  # 28个交叉注意力层\n",
    "\n",
    "# 这里的encoder_hidden_states还是作为k,v的输入\n",
    "projection_matrices = [l.to_v for l in ca_layers]\n",
    "og_matrices = [copy.deepcopy(l.to_v) for l in ca_layers]\n",
    "if is_use_k:\n",
    "    projection_matrices = projection_matrices + [l.to_k for l in ca_layers]\n",
    "    og_matrices = og_matrices + [copy.deepcopy(l.to_k) for l in ca_layers]\n",
    "print(projection_matrices)\n",
    "\n",
    "# 这里面很重要的一部分，ca_layers就是attention\n",
    "# 重置参数,这部分可以修改一下\n",
    "num_caption_layer = len(ca_layers)\n",
    "for idx, layer in enumerate(ca_layers):\n",
    "    layer.to_v = copy.deepcopy(projection_matrices[idx])\n",
    "    projection_matrices[idx] = layer.to_v\n",
    "    if is_use_k:\n",
    "        layer.to_k = copy.deepcopy(og_matrices[num_caption_layer+idx])\n",
    "        projection_matrices[num_caption_layer+idx] = layer.to_k\n",
    "        \n",
    "old_texts = []\n",
    "new_texts = []\n",
    "for old_text, new_text in zip(old_texts_, new_texts_):\n",
    "    old_texts.append(old_text)\n",
    "    n_t = new_text\n",
    "    if n_t == '':\n",
    "        n_t = ' '\n",
    "    new_texts.append(n_t)\n",
    "\n",
    "lamb = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 重新写一个概念映射\n",
    "# for layer_num in range(len(projection_matrices)):\n",
    "#     with torch.no_grad():\n",
    "#         position1 = torch.zeros((1152, 1152)).to(device)\n",
    "#         position2 = torch.zeros((1152, 1152)).to(device)\n",
    "#         position3 = torch.zeros((1152, 1152)).to(device)\n",
    "#         position4 = torch.zeros((1152, 1152)).to(device)\n",
    "\n",
    "#         for old_text, new_text in zip(old_texts, new_texts):\n",
    "#             # 获取missi c^T\n",
    "#             new_embeds_cap_spatial = get_prompt_embedding(videogen_pipeline, new_text)  # [3,1152]\n",
    "#             # vi\n",
    "#             old_embeds_cap_spatial = get_prompt_embedding(videogen_pipeline, old_text)\n",
    "#             layer = projection_matrices[layer_num]\n",
    "#             vi = layer(new_embeds_cap_spatial).detach()  # [3,1152]\n",
    "            \n",
    "#             position1 += old_embeds_cap_spatial.transpose(0, 1) @ vi\n",
    "            \n",
    "            \n",
    "#             position3 += old_embeds_cap_spatial.transpose(0, 1) @ old_embeds_cap_spatial\n",
    "        \n",
    "#         for remain_text in ret_texts:\n",
    "#             remain_embeds_cap_spatial = get_prompt_embedding(videogen_pipeline, remain_text)\n",
    "#             layer = projection_matrices[layer_num]\n",
    "#             vi = layer(remain_embeds_cap_spatial).detach()\n",
    "#             position2 += remain_embeds_cap_spatial.transpose(0, 1) @ vi\n",
    "#             position4 += remain_embeds_cap_spatial.transpose(0, 1) @ remain_embeds_cap_spatial\n",
    "#         mat1 = position1 + position2\n",
    "#         mat2 = position3 + position4\n",
    "        \n",
    "#         projection_matrices[layer_num].weight = torch.nn.Parameter(mat1 @ torch.inverse(mat2))\n",
    "#         if layer_num == 0:\n",
    "#             print(mat1)\n",
    "#             print(mat2)\n",
    "#             print(projection_matrices[layer_num].weight)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# ans = mat2.cpu().numpy()\n",
    "# ans\n",
    "# def is_full_rank_matrix_C(A):\n",
    "#     # 计算矩阵的行阶梯形式\n",
    "#     rref = np.linalg.matrix_rank(A)\n",
    "#     return rref == A.shape[0]\n",
    "\n",
    "# print(is_full_rank_matrix_C(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下一步的修改计划：使用直接对齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n",
      "t('Biden is running', 'Messi is running')\n",
      "old_embeds_cap_spatialtorch.Size([5, 1152]),new_embeds_cap_spatialtorch.Size([5, 1152])\n"
     ]
    }
   ],
   "source": [
    "for layer_num in range(len(projection_matrices)):\n",
    "    with torch.no_grad():\n",
    "        # 不改变梯度, 这是后面推导的公式\n",
    "        mat1 = lamb * projection_matrices[layer_num].weight\n",
    "        mat2 = lamb * torch.eye(projection_matrices[layer_num].weight.shape[1], device = projection_matrices[layer_num].weight.device)\n",
    "        for idx, t in enumerate(zip(old_texts, new_texts)):\n",
    "            print(f't{t}')\n",
    "            old_text, new_text = t\n",
    "            old_embeds_cap_spatial = get_prompt_embedding(videogen_pipeline, old_text)\n",
    "            new_embeds_cap_spatial = get_prompt_embedding(videogen_pipeline, new_text)\n",
    "            print(f'old_embeds_cap_spatial{old_embeds_cap_spatial.shape},new_embeds_cap_spatial{new_embeds_cap_spatial.shape}')\n",
    "            context = old_embeds_cap_spatial.detach()\n",
    "            # print(f'context{context.shape}')\n",
    "            values = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # for layer in projection_matrices:\n",
    "                #     values.append(layer(new_embeds_cap_spatial.detach()))\n",
    "                for layer in projection_matrices:\n",
    "                    o_embs = layer(old_embeds_cap_spatial).detach()  # v*\n",
    "                    u = o_embs\n",
    "                    u = u / u.norm()\n",
    "                    \n",
    "                    new_embs = layer(new_embeds_cap_spatial).detach()\n",
    "                    new_embs_proj = (u*new_embs).sum()\n",
    "                    \n",
    "                    traget = new_embs - (new_embs_proj * u)\n",
    "                    values.append(traget.detach())\n",
    "                    \n",
    "            \n",
    "            context_vector = context # c [token,1152]\n",
    "            context_vector_T = context.transpose(0, 1)  # [1152, token]\n",
    "            \n",
    "            value_vector = values[layer_num]\n",
    "            for_mat1 = (context_vector_T @ value_vector)  # [1152,1152]\n",
    "            for_mat2 = (context_vector_T @ context_vector)  # [1152,1152]\n",
    "            # print(f'format1{for_mat1.shape},format2{for_mat2.shape},mat1{mat1.shape}')\n",
    "            mat1 += erase_scale*for_mat1\n",
    "            mat2 += erase_scale*for_mat2\n",
    "            \n",
    "        # # 对于其他要保存的参数，我们要保存下来\n",
    "        # for old_text, new_text in zip(ret_texts, ret_texts):\n",
    "        #     print(f'old_text{old_text},new_text{new_text}')\n",
    "        #     old_embeds_cap_spatial = get_prompt_embedding(videogen_pipeline, old_text)\n",
    "        #     new_embeds_cap_spatial = get_prompt_embedding(videogen_pipeline, new_text)\n",
    "            \n",
    "            \n",
    "        #     context = old_embeds_cap_spatial.detach()\n",
    "        #     values = []\n",
    "            \n",
    "        #     with torch.no_grad():\n",
    "        #         for layer in projection_matrices:\n",
    "        #             values.append(layer(new_embeds_cap_spatial.detach()))\n",
    "        #     context_vector = context # c [32,120,1152]\n",
    "        #     context_vector_T = context.transpose(0, 1)  # cT  [32,1152,120] []\n",
    "            \n",
    "        #     value_vector = values[layer_num]\n",
    "        #     for_mat1 = (context_vector_T @ value_vector)  # [1152,1152]\n",
    "        #     for_mat2 = (context_vector_T @ context_vector)\n",
    "        #     mat1 += preserve_scale*for_mat1\n",
    "        #     mat2 += preserve_scale*for_mat2\n",
    "        \n",
    "        projection_matrices[layer_num].weight = torch.nn.Parameter((mat1 @ torch.inverse(mat2)))\n",
    "        \n",
    "checkpoint = { \"model\": lattet2v_model.state_dict() }\n",
    "torch.save(checkpoint, 'lattet2v_model2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0272, -0.0230,  0.0790,  ..., -0.0552, -0.0358,  0.0142],\n",
      "        [-0.0074,  0.0244, -0.0025,  ..., -0.0375, -0.0101,  0.0196],\n",
      "        [ 0.0248, -0.0945,  0.0197,  ...,  0.0168,  0.0602, -0.0135],\n",
      "        ...,\n",
      "        [-0.0385,  0.0033, -0.0467,  ..., -0.0477,  0.0311, -0.0164],\n",
      "        [ 0.0092, -0.0142, -0.0447,  ..., -0.0395,  0.0160, -0.0565],\n",
      "        [-0.0242,  0.0240, -0.0378,  ..., -0.0009, -0.0251,  0.0126]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0272, -0.0230,  0.0790,  ..., -0.0552, -0.0358,  0.0142],\n",
      "        [-0.0074,  0.0244, -0.0025,  ..., -0.0375, -0.0101,  0.0196],\n",
      "        [ 0.0248, -0.0945,  0.0197,  ...,  0.0168,  0.0602, -0.0135],\n",
      "        ...,\n",
      "        [-0.0385,  0.0033, -0.0467,  ..., -0.0477,  0.0311, -0.0164],\n",
      "        [ 0.0092, -0.0142, -0.0447,  ..., -0.0395,  0.0160, -0.0565],\n",
      "        [-0.0242,  0.0240, -0.0378,  ..., -0.0009, -0.0251,  0.0126]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0270, -0.0089,  0.0713,  ..., -0.0623, -0.0364,  0.0031],\n",
      "        [-0.0012,  0.0181,  0.0084,  ..., -0.0404,  0.0020,  0.0161],\n",
      "        [ 0.0142, -0.0662, -0.0083,  ..., -0.0006,  0.0634, -0.0402],\n",
      "        ...,\n",
      "        [-0.0273, -0.0215, -0.0196,  ..., -0.0344,  0.0239,  0.0042],\n",
      "        [ 0.0031, -0.0113, -0.0506,  ..., -0.0469,  0.0275, -0.0688],\n",
      "        [-0.0072,  0.0164,  0.0131,  ...,  0.0002,  0.0067,  0.0363]],\n",
      "       device='cuda:1', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 这里面我要确认是否真的改变参数\n",
    "l_to_v_old = ca_layers[0].to_v.weight\n",
    "print(l_to_v_old)\n",
    "l_to_v_new = projection_matrices[0].weight\n",
    "print(l_to_v_new)\n",
    "og_to_v = og_matrices[0].weight\n",
    "print(og_to_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latte2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
